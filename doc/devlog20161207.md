# 2016-12-12 Mon. to 2016-12-18 Sun.

## TODO
- :star:学習時間の計測を実装
- :star:バッチを10、学習率を0.01にしてFizzBuzzの精度を上げる
- DLFizzBuzzに詳しいコメントを入れる
- FizzBuzzブログ化
- 学習曲線を表示する
- MNIST予想のみのファイルを作成
- MNISTブログ化、97%をプーリング層なしで出す
- dropoutを取り入れる
- 数値パラメーターを範囲内で色々設定できるようにする
- Yのactivation functionを''のみにする
- パラメーターを数値のみで選択できるように整理する
- Daily and SportsでDLの実験
- CIFAR-10でDLの実験
- DEAPの再学習
- DEAPのブログ化
- 交差エントロピーを具体的な数値で理解
- 最急降下法を具体的な数値で理解
- TensorFlowの各設計の調査
- TensorFlowの各設計のブログ化
- TensorFlowのロス関数、最適化関数の調査
- TensorFlowのロス関数、最適化関数のブログ化
- FizzBuzzのブログを書いて下書き保存
- モデルを保存して、それを利用した精度検証と実例の提示


FizzBuzz問題とは何か。
[こちら](http://d.hatena.ne.jp/keyword/Fizz-Buzz%CC%E4%C2%EA)から引用させてもらう。

> Fizz-Buzz問題の例はこんな感じだ。  
> 1から100までの数をプリントするプログラムを書け。ただし3の倍数のときは数の代わりに「Fizz」と、5の倍数のときは「Buzz」とプリントし、3と5両方の倍数の場合には「FizzBuzz」とプリントすること。  
> ちゃんとしたプログラマであれば、これを実行するプログラムを2分とかからずに紙に書き出せるはずだ。怖い事実を聞きたい? コンピュータサイエンス学科卒業生の過半数にはそれができないのだ。自称上級プログラマが答えを書くのに10-15分もかかっているのを見たこともある。

これをDeep Learningで学習したモデルに行わせる。
なぜDeep Learning入門用ベンチマークかと言うと、
教師データの101〜1023とテストデータの0〜100の作成コードが、ラベリングも含めて簡単に実装できるし、
モデルの設計も隠れ層が１つとシンプルなので速く収束し、そこそこ高い精度が出せるから。
データ作成とDeep Learningの実装は[TensorFlowコトハジメ Fizz-Buzz問題](http://yaju3d.hatenablog.jp/entry/2016/06/05/234113)が詳しい。
以下では同様にTensorFlowを使っているが、実装やパラメーターが異なるし、説明は先のリンクの方が丁寧なので、詳しくない方は読んでおくことをおすすめする。


### フロー
main関数の3ステップがフローとなる。

```python
class DLFizzBuzz:
    def main(self):
        # 1. FizzBuzzデータを生成して取得する。
        data = DataFizzBuzz().main()

        # 2. Deep Learnigモデルを設計する。
        model = self.design_model(data)

        # 3. Deep Learningモデルを学習させる。
        self.train_model(data, model)
```


### データ生成

生成コードは以下の通り。
main()を呼び出すと教師データ、教師ラベル、テストデータ、テストラベルをリストで取得できる。

```python
BORDER = 101
NUM_DIGITS = 10

class DataFizzBuzz:
    def main(self):
        # Train
        train_data = np.array([self.binary_encode(i, NUM_DIGITS) for i in range(BORDER, 2**NUM_DIGITS)])
        train_label = np.array([self.fizz_buzz_encode(i) for i in range(BORDER, 2**NUM_DIGITS)])

        # Test
        test_data = np.array([self.binary_encode(i, NUM_DIGITS) for i in range(0, BORDER)])
        test_label = np.array([self.fizz_buzz_encode(i) for i in range(0, BORDER)])

        # Collect
        data = [train_data, train_label, test_data, test_label]

        return data


    def binary_encode(self, i, num_digit):
        binary = np.array([i >> d & 1 for d in range(NUM_DIGITS)])

        return binary


    def fizz_buzz_encode(self, i):
        if i % 15 == 0:
            result = np.array([0, 0, 0, 1])
        elif i % 5 == 0:
            result = np.array([0, 0, 1, 0])
        elif i % 3 == 0:
            result = np.array([0, 1, 0, 0])
        else:
            result = np.array([1, 0, 0, 0])

        return result
```


### モデル設計


### モデル学習


### 結果
$ python dl_fizzbuzz.py
Epoch: 9900, 	 Loss: 0.00862701, 	 Train Accracy: 1.0, 	 Test Accracy: 0.920792

$ python dl_fizzbuzz.py
Epoch: 9900, 	 Loss: 0.00208146, 	 Train Accracy: 1.0, 	 Test Accracy: 0.970297


# 2016-12-07 Wed. to 2016-12-11 Sun.

## TODO
- :star:パラメーターの値はすべて整数にして具体的な数は中で決めることにする
- :star:数値パラメーターを文字パラメーターで指定できるようにする

## パラメーター

```
- layer
    - n_hidden_layer: 0 ~ 4
    - n_node: 5 ~ 100
    - weight: 'zeros', 'ones', 'random_normal', 'truncated_normal'
    - stddev: 0.0001 ~ 0.1
    - bias: 'zeros', 'ones'
    - activ_func: '', 'relu', 'tanh', 'softmax'

- trainer
    - optimizer: GradientDescentOptimizer, AdamOptimizer
    - learning rate: 0.0001 ~ 0.1

- batch size: 10 ~ 100
- num iter: 1 ~ 10000

[100], [['random_normal', 0.01, 'zeros', 'relu']], [GradientDescentOptimizer, 0.05, 100, 10000]
```
