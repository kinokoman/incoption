# 2016-12-07 Wed. to 2016-12-11 Sun.

## TODO
- :star:パラメーターの値はすべて整数にして具体的な数は中で決めることにする
- :star:数値パラメーターを文字パラメーターで指定できるようにする
- FizzBuzzブログ化
- 数値パラメーターを範囲内で色々設定できるようにする
- MNISTブログ化、97%をプーリング層なしで出す
- パラメーターを数値のみで選択できるように整理する
- Daily and SportsでDLの実験
- CIFAR-10でDLの実験
- DEAPの再学習
- DEAPのブログ化
- 交差エントロピーを具体的な数値で理解
- 最急降下法を具体的な数値で理解
- TensorFlowの各設計の調査
- TensorFlowの各設計のブログ化
- TensorFlowのロス関数、最適化関数の調査
- TensorFlowのロス関数、最適化関数のブログ化
- FizzBuzzのブログを書いて下書き保存
- モデルを保存して、それを利用した精度検証と実例の提示


## パラメーター

```
- layer
    - n_hidden_layer: 0 ~ 4
    - n_node: 5 ~ 100
    - weight: 'zeros', 'ones', 'random_normal', 'truncated_normal'
    - stddev: 0.0001 ~ 0.1
    - bias: 'zeros', 'ones'
    - activ_func: '', 'relu', 'tanh', 'softmax'

- trainer
    - optimizer: GradientDescentOptimizer, AdamOptimizer
    - learning rate: 0.0001 ~ 0.1

- batch size: 10 ~ 100
- num iter: 1 ~ 10000

[100], [['random_normal', 0.01, 'zeros', 'relu']], [GradientDescentOptimizer, 0.05, 100, 10000]
```

最初に層の数を指定する。

```python
node_params = [50, 50]

model_params = [
    {
        'weight' = 'random_normal'
        'stddev' = 0.01
        'bias' = 'zeros'
        'activ' = 'relu'
    },
    {
        'weight' = 'random_normal'
        'stddev' = 0.01
        'bias' = 'zeros'
        'activ' = 'relu'
    },
   {
        'weight' = 'random_normal'
        'stddev' = 0.01
        'bias' = 'zeros'
        'activ' = ''
    }

other_params = {
    'trainer': 0
    'learnig_rate': 0.01
    'batch_size': 10
    'n_iter': 10
}


# FizzBuzz
X  = tf.placeholder(tf.float32, [None, n_X])
H1 = self.__make_layer(X, n_X, n_hidden[0], 'random_normal', 'zeros', 'relu')
Y  = self.__make_layer(H1, n_hidden[0], n_Y, 'random_normal', 'zeros', '')
Y_ = tf.placeholder(tf.float32, [None, n_Y])
```
